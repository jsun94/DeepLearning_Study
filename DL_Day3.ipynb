{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 합성곱 계층\n",
    "\n",
    "- 윈도(window / filter)라는 가중치의 유닛이 이전 계츠으이 모여 있는 일정한 수의 유닛에 연결된다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Convolution Layer\n",
    "- Fully Connected가 아니라 일부만 연결한 것\n",
    "- Fully Connected = Dense"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 활성함수 (Activation Function)\n",
    "- sigmoid\n",
    "- -1에서 1 사이의 값을 출력 = tanh:\n",
    "    - 음수값이 유호하면 sigmoid를 쓰지 말고 tanh를 사용하라\n",
    "- ReLu:\n",
    "    - sigmoid 함수는 미분이 반복될수록 0에 가까워 진자\n",
    "    - 따라서 층이 깊어질수록 sigmoid를 사용할 경우 끝에 있는 것은 영향력이 거의 0에 수렴한다.\n",
    "    - 이에 따라 상한선을 제거 -> ReLu\n",
    "    - ReLu를 쓰게 되면 마이너스를 다룰 수 없다.\n",
    "- Leaky ReLu:\n",
    "    - ReLu에서 마이너스를 유효하게 한 것.\n",
    "    - 마이너스 구간의 기울기는 가파르지 않다.\n",
    "- Maxout:\n",
    "    - 둘 중에 무조건 큰 것을 도출\n",
    "- ELU\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- softmax:\n",
    "    - class가 여러개 일 때 결과를 각각의 확률을 따져 제일 높은 확률로 나온 것이 정답인 것으로 도출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior()\n",
    "tf.set_random_seed(777)\n",
    "\n",
    "x_data = [[1,2,1,1],\n",
    "         [2,1,3,2],\n",
    "         [3,1,3,4],\n",
    "         [4,1,5,5],\n",
    "         [1,7,5,5],\n",
    "         [1,2,5,6],\n",
    "         [1,6,6,6],\n",
    "         [1,7,7,7]]\n",
    "\n",
    "y_data = [[0,0,1],\n",
    "         [0,0,1],\n",
    "         [0,0,1],\n",
    "         [0,1,0],\n",
    "         [0,1,0],\n",
    "         [0,1,0],\n",
    "         [1,0,0],\n",
    "         [1,0,0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tf.placeholder('float',[None,4])\n",
    "Y = tf.placeholder('float',[None,3]) #다중 클래스\n",
    "nb_classes = 3\n",
    "W = tf.Variable(tf.random_normal([4,nb_classes]), name = 'weight') #Y가 (n,3)이므로 (4,3)\n",
    "b = tf.Variable(tf.random_normal([nb_classes]), name = 'bias')\n",
    "\n",
    "#softmax = exp(logits) / reduce_sum(exp(logits),dim)\n",
    "hypothesis = tf.nn.softmax(tf.matmul(X, W) + b)\n",
    "\n",
    "cost = tf.reduce_mean(-tf.reduce_sum(Y * tf.log(hypothesis), axis = 1))\n",
    "\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.1).minimize(cost)\n",
    "\n",
    "\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    for step in range(2001):\n",
    "        sess.run(optimizer, feed_dict = {X:x_data, Y:y_data})\n",
    "        if step % 200 ==0:\n",
    "            print(step, sess.run(cost, feed_dict = {X:x_data, Y:y_data}))\n",
    "    a = sess.run(hypothesis, feed_dict = {X : [[1,11,7,9]]})\n",
    "    print(a,sess.run(tf.argmax(a,1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 역전파 알고리즘:\n",
    "    - 말 그대로 거꾸로 거슬러 올라가 가중치를 수정하는 것, 각 과정마다 편미분을 하여\n",
    "    - 영향을 끼치는 정도를 알아낸다\n",
    "- 드롭아웃:\n",
    "    - 일반적으로 Fully Connected로 되어 있었다.\n",
    "    - -> 이것을 랜덤하게 연결하여 보자(Dropout)\n",
    "    - keras나 tensorflow에서 dropout의 파라미터가 살짝씩 다르다 (In Out)\n",
    "    - batch 때마다 적용이 된다.\n",
    "    - 검증을 할 떄는 Dropout =0 , 즉 적용이 되어서는 안된다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = [[1],[3],[0]]\n",
    "Y_one_hot = tf.one_hot(Y,4)\n",
    "print(Y_one_hot)\n",
    "#Y_one_hot.shape = (3,1,4)\n",
    "#네개 짜리 데이터의 한 묶음이 세개가 있다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.loadtxt('C:/Users/edu/venv/Scripts/data-04-zoo.csv', delimiter = ',',skiprows = 1, dtype = np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_data = data[:,0:-1] \n",
    "y_data = data[:,[-1]] \n",
    "\n",
    "nb_classes = 7\n",
    "X = tf.placeholder(tf.float32, shape = (None,16))\n",
    "W = tf.Variable(tf.random_normal([16,nb_classes]), name = 'weight')\n",
    "b = tf.Variable(tf.random_normal([nb_classes]), name = 'bias')\n",
    "Y = tf.placeholder(tf.int32, shape = (None,1))\n",
    "Y_one_hot = tf.one_hot(Y,nb_classes)\n",
    "Y_one_hot = tf.reshape(Y_one_hot, [-1,nb_classes])\n",
    "print('reshape : ', Y_one_hot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x_data.shape)\n",
    "print(y_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits = tf.matmul(X,W) + b\n",
    "\n",
    "hypothesis = tf.nn.softmax(logits)\n",
    "\n",
    "cost_i = tf.nn.softmax_cross_entropy_with_logits(logits = logits, labels = Y_one_hot)\n",
    "\n",
    "cost = tf.reduce_mean(cost_i)\n",
    "\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.1).minimize(cost)\n",
    "\n",
    "prediction = tf.argmax(hypothesis,1)\n",
    "correct_prediction = tf.equal(prediction, tf.argmax(Y_one_hot,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    for step in range(2001):\n",
    "        sess.run(optimizer, feed_dict = {X:x_data, Y:y_data})\n",
    "        if step % 200 ==0:\n",
    "            loss, acc = sess.run([cost, accuracy],feed_dict = {X : x_data, Y:y_data})\n",
    "            print('step : {:5}\\tLoss : {:.3f}\\tAcc : {: .2%}'.format(step, loss, acc))\n",
    "    pred = sess.run(predcition, feed_dict = {X : x_data})\n",
    "    for p,y in zip(pred, y_data.flatten()):\n",
    "        print(\"[{}] Prediction : {} True Y : {}\".format(p == int(y), p, int(y)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
