{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- optimizer /// 지각\n",
    "- minmax 보다 z-score가 데이터의 왜곡이 적다"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- CNN\n",
    "    - Conv -  ReLu - Conv - ReLu - POOL x 3 + FullyConnected Layer\n",
    "- Convolutional Layer\n",
    "    - 특정 구간의 대표값을 뽑아내는 과정 (특징)\n",
    "- Moving Filter\n",
    "    - 전체 이미지에서 우리가 구간을 3x3이라고 정한다.\n",
    "    - stride : 필터를 움직이는 범위\n",
    "    - 7x7에서 진행할 경우 output = 5x5가 된다.\n",
    "- Zero Padding:\n",
    "    - 의미 없는 부분을 0으로 채워 넣는다.\n",
    "    - 0은 검은색이지만 어떤 weight를 곱해도 0이므로 \n",
    "    - Zero padding을 해도 stride =2 이면 이미지 사이즈가 줄어드는 것은 어쩔 수 없다.\n",
    "    - stride =1 일 때 이미지 사이즈를 유지\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- CNN 구현 예\n",
    "    - 필터의 사이즈를 결정한다.\n",
    "    - 필터의 stride를 결정한다.\n",
    "    - zero-padding을 쓸지 말지를 결정한다.\n",
    "    - 필터의 갯수를 결정한다. -> 필터의 갯수가 많아지면 이미지의 여러가지 특징을 도출할 가능성이 높아진다.\n",
    "    - 필터의 각 값들은 랜덤하게 뽑아내므로 학습 결과가 같게 나오지 않는다. \n",
    "- POOLING Layer\n",
    "    - 특징들을 추려내는 과정\n",
    "    - 이미지를 줄인다.\n",
    "    - Max Poolling : \n",
    "        - 각 필터의 결과로 나온 것에서 가장 큰 값을 도출한다.\n",
    "        - stride, zero-padding 쓸지 말지 (추려내는 것이므로 일반적으로 zero-padding 사용 x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.compat.v1 as tf\n",
    "mnist = tf.keras.datasets.mnist\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior()\n",
    "import matplotlib.pyplot as plt\n",
    "tf.set_random_seed(777)\n",
    "\n",
    "print(x_train.shape)\n",
    "#x_train에는 이미지의 정보"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(x_train[0])\n",
    "plt.show()\n",
    "print(y_train.shape)\n",
    "print(y_train[0])\n",
    "#y_train에는 라벨이 들어가 있음을 확인할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_class = 10 #0~9\n",
    "\n",
    "X = tf.placeholder(tf.float32, [None,784]) #28x28 = 784\n",
    "Y = tf.placeholder(tf.int32, [None, 1])\n",
    "\n",
    "W = tf.Variable(tf.random_normal([784,nb_class]), name = 'weight')\n",
    "b = tf.Variable(tf.random_normal([nb_class]))\n",
    "\n",
    "X_re = x_train.reshape(-1,784)\n",
    "# X_r = tf.cast(X_re,tf.float32)\n",
    "#print(X_r)\n",
    "print(X_re.shape)\n",
    "Y_one_hot = tf.one_hot(Y, nb_class)\n",
    "# print(Y_one_hot)\n",
    "Y_one_hot = tf.reshape(Y_one_hot, [-1,nb_class])\n",
    "Y_t_re = y_train.reshape(-1,1)\n",
    "print(Y_t_re.shape)\n",
    "# print(Y_one_hot)\n",
    "#print(y_train.shape)\n",
    "# print(x_train.shape)\n",
    "\n",
    "\n",
    "logits = tf.matmul(X,W) + b\n",
    "\n",
    "hypothesis = tf.nn.softmax(logits)\n",
    "\n",
    "cost_i = tf.nn.softmax_cross_entropy_with_logits(logits = logits, labels = Y_one_hot)\n",
    "\n",
    "cost = tf.reduce_mean(cost_i)\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.01).minimize(cost_i)\n",
    "\n",
    "prediction = tf.argmax(hypothesis,1)\n",
    "\n",
    "correct_prediction = tf.equal(prediction, tf.argmax(Y_one_hot,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    for step in range(2001):\n",
    "        sess.run(optimizer, feed_dict = {X:X_re, Y:Y_t_re})\n",
    "        if step % 400 ==0:\n",
    "            loss, acc = sess.run([cost, accuracy],feed_dict = {X : X_re, Y:Y_t_re})\n",
    "            print('step : {:5}\\tLoss : {:.3f}\\tAcc : {: .2%}'.format(step, loss, acc))\n",
    "    pred = sess.run(prediction, feed_dict = {X : x_test.reshape(-1,784)})\n",
    "    for p,y in zip(pred, y_test.flatten()):\n",
    "#         print(\"[{}] Prediction : {} True Y : {}\".format(p == int(y), p, int(y)))\n",
    "        ind +=1\n",
    "        if p != int(y):\n",
    "            print(\"[{}] Prediction : {} True  Y :{}\".format(p == int(y),p,int(y)))\n",
    "            plt.imshow(x_test[ind])\n",
    "            plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CNN\n",
    "import tensorflow as tf\n",
    "tf.__version__\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "tf.set_random_seed(777)\n",
    "\n",
    "mnist = input_data.read_data_sets('MNIST_data/',one_hot = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.001\n",
    "training_epochs = 15\n",
    "batch_size = 100\n",
    "\n",
    "X = tf.placeholder(tf.float32, [None,784])\n",
    "X_img = tf.reshape(X, [-1,28,28,1]) #MNIST는 흑백이라서 1\n",
    "Y = tf.placeholder(tf.float32, [None, 10])\n",
    "keep_prob = tf.palceholder\n",
    "#stddev는 표준편차를 0.01로 줄인 것\n",
    "#X_img와 맞춰주기 위해 3x3에 1 채널\n",
    "#32는 필터의 갯수\n",
    "W1 = tf.Variable(tf.random_normal([3,3,1,32], stddev = 0.01)) \n",
    "#패딩사용 (SAME)\n",
    "#더 고차원이 되면 필요한 strides의 4개의 파라미터\n",
    "#현재는 strides에서 가운데 두개의 인자만 인식하면 됌 1,1만큼 움직이겠다\n",
    "L1 = tf.nn.conv2d(X_img, W1, strides = [1,1,1,1], padding = 'SAME')\n",
    "L1 = tf.nn.relu(L1)\n",
    "L1 = tf.nn.dropout(L1, keep_prob = keep_prob) #렐루에서 가져온 가중치를 70%만 활성화\n",
    "#맥스풀에서 커널 사이즈는 2x2에 이동은 두칸씩한다는 의미이다.\n",
    "L1 = tf.nn.max_pool(L1, ksize = [1,2,2,1], strides = [1,2,2,1], padding = 'SAME')\n",
    "#size = 14x14\n",
    "\n",
    "#위에서 32개의 필터를 사용했으므로 L1(conv2d)의 결과 32겹의 필터가 생성되므로\n",
    "#이것을 겹쳐서 depth = 32가 된다.\n",
    "W2 = tf.Variable(tf.random_normal([3,3,32,64], stddev = 0.01)) \n",
    "L2 = tf.nn.conv2d(L1, W2, strides = [1,1,1,1], padding = 'SAME')\n",
    "L2 = tf.nn.relu(L2)\n",
    "L2 = tf.nn.max_pool(L2, ksize = [1,2,2,1], strides = [1,2,2,1], padding='SAME')\n",
    "#size = 7x7\n",
    "\n",
    "L2_flat = tf.reshape(L2, [-1, 7*7*64])\n",
    "W3 = tf.Variable(tf.random_normal([7*7*64, 10]))\n",
    "b = tf.Variable(tf.random_normal([10]))\n",
    "\n",
    "logits = tf.matmul(L2_flat , W3) + b\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits = logits, labels = Y))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate = learning_rate).minimize(cost)\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "print(\"Learning started. It takes sometimes.\")\n",
    "#epoch : 같은 데이터를 반복적으로 학습\n",
    "#batch_size : 작으면 데이터 하나하나에 민감하기 때문에 Overfitting될 확률이 커진다.\n",
    "# 야구 축구 농구 배구 같은 것을 분류하려면 세세한 데이터를 따져야 하므로 batch_size가 작은게 좋다\n",
    "# 하지만 야구, 야구장, 야구공과 같은 것을 분류하는 것이라면 batch_size가 큰 것이 좋다.\n",
    "for epoch in range(training_epochs):\n",
    "    avg_cost = 0\n",
    "    total_batch = int(mnist.train.num_examples / batch_size)\n",
    "    \n",
    "    for i in range(total_batch):\n",
    "        batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "        feed_dict = {X:batch_xs, Y:batch_ys, keep_prob : 0.7}\n",
    "        c,_ = sess.run([cost, optimizer], feed_dict = feed_dict)\n",
    "        avg_cost += c/total_batch\n",
    "        \n",
    "    print(\"Epoch :\", '%04d' %(epoch + 1), 'cost = ', '{:.9f}'.format(avg_cost))\n",
    "print(\"Learning Done\")\n",
    "\n",
    "correct_prediction = tf.equal(tf.argmax(logits,1),tf.argmax(Y,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "print(\"Accuracy : \", sess.run(accuracy, feed_dict = \\\n",
    "                              {X : mnist.test.imgaes, Y: mnist.test.labels, keep_prob : 0.7}))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
